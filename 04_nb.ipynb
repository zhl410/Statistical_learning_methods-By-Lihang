{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 参数估计中的频率主义学派和贝叶斯学派\n",
    " \n",
    " 事实上,概率模型的训练过程就是参数估计(parameter estimation)过程对于参数估计,统计学界的两个学派分别提供了不同的解决方案:频率主义学派(Frequentist)认为参数虽然未知,但却是客观存在的固定值,因此,可通过优化似然函数等准则来确定参数值;贝叶斯学派(Bayesian)则认为参数是未观察到的随机变量,其本身也可有分布,因此,可假定参数服从一个先验分布,然后基于观测到的数据来计算参数的后验分布.本节介绍源自频率主义学派的极大似然估计(Maximum Likelihood Estimation,简称MLE),这是根据数据采样来估计概率分布参数的经典方法."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the log of the sum of exponentials of input elements\n",
    "from scipy.special import  logsumexp\n",
    "#View inputs as arrays with at least two dimensions.\n",
    "#np.atleast_2d\n",
    "from sklearn import datasets\n",
    "#from sklearn.naive_bayes import GaussianNB\n",
    "import numpy as np\n",
    "from abc import ABCMeta, abstractmethod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BaseNB <- GaussianNB\n",
    "\n",
    "假设类条件概率服从高斯分布，其概率密度函数为：\n",
    "$$P(x_i | y_k)=\\frac{1}{\\sqrt{2\\pi\\sigma^2_{yk}}}exp(-\\frac{(x_i-\\mu_{yk})^2}{2\\sigma^2_{yk}})$$\n",
    "数学期望(mean)：$\\mu$，方差：$\\sigma^2=\\frac{\\sum(X-\\mu)^2}{N}$\n",
    "\n",
    "可以利用MLE对$\\mu$和$\\sigma^2$进行参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class BaseNB(metaclass=ABCMeta):\n",
    "    \n",
    "    @abstractmethod\n",
    "    def _joint_log_likelihood(self, X):\n",
    "        \"\"\"\n",
    "        计算没有被归一化的后验概率的对数 I.e. ``log P(c) + log P(x|c)``, 右边分子部分\n",
    "        Returns: [n_sample, n_classes]\n",
    "        \"\"\"\n",
    "        pass\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        X : array-like, shape = [n_samples, n_features]\n",
    "        \"\"\"\n",
    "        jll = self._joint_log_likelihood(X)\n",
    "        return self.classes_[np.argmax(jll, axis=1)]\n",
    "\n",
    "    def predict_log_proba(self, X):\n",
    "        \"\"\"\n",
    "        X : array-like, shape = [n_samples, n_features]\n",
    "        Returns : array-like, shape = [n_samples, n_classes]\n",
    "        \"\"\"\n",
    "        jll = self._joint_log_likelihood(X)\n",
    "        log_prob_x = logsumexp(jll, axis=1)\n",
    "        return jll - np.atleast_2d(log_prob_x).T\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        X : array-like, shape = [n_samples, n_features]\n",
    "        Returns : array-like, shape = [n_samples, n_classes]\n",
    "        \"\"\"\n",
    "        return np.exp(self.predict_log_proba(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNB(BaseNB):\n",
    "    def __init__(self, priors=None, var_smoothing=1e-9):\n",
    "        self.priors = priors\n",
    "        self.var_smoothing = var_smoothing\n",
    "    def _get_mean_variance(self, X, sample_weight=None):\n",
    "        # Compute (potentially weighted) mean and variance of new datapoints\n",
    "        if sample_weight is not None:\n",
    "            n_new = float(sample_weight.sum())\n",
    "            new_mu = np.average(X, axis=0, weights=sample_weight / n_new)\n",
    "            new_var = np.average((X - new_mu) ** 2, axis=0,\n",
    "                                 weights=sample_weight / n_new)\n",
    "        else:\n",
    "            n_new = X.shape[0]\n",
    "            new_var = np.var(X, axis=0)\n",
    "            new_mu = np.mean(X, axis=0)\n",
    "\n",
    "        return new_mu, new_var\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        # If the ratio of data variance between dimensions is too small, it\n",
    "        # will cause numerical errors. To address this, we artificially\n",
    "        # boost the variance by epsilon, a small fraction of the standard\n",
    "        # deviation of the largest dimension.\n",
    "        self.epsilon_ = self.var_smoothing * np.var(X, axis=0).max()\n",
    "        self.classes_ = np.unique(y)\n",
    "        n_features = X.shape[1]\n",
    "        n_classes = len(self.classes_)\n",
    "        self.theta_ = np.zeros((n_classes, n_features))\n",
    "        self.sigma_ = np.zeros((n_classes, n_features))\n",
    "        self.class_count_ = np.zeros(n_classes, dtype=np.float64)\n",
    "\n",
    "        for (i, y_i) in enumerate(self.classes_):\n",
    "            X_i = X[y == y_i, :]\n",
    "            if sample_weight is not None:\n",
    "                sw_i = sample_weight[y == y_i]\n",
    "                N_i = sw_i.sum()\n",
    "            else:\n",
    "                sw_i = None\n",
    "                N_i = X_i.shape[0]\n",
    "            self.theta_[i, :], self.sigma_[i, :] = self._get_mean_variance(X_i, sw_i)\n",
    "            self.class_count_[i] += N_i\n",
    "        self.sigma_[:, :] += self.epsilon_\n",
    "\n",
    "        # Set priors \n",
    "        if self.priors:\n",
    "            self.class_prior_ = np.asarray(self.priors)\n",
    "        else:\n",
    "            # Empirical prior, with sample_weight taken into account\n",
    "            self.class_prior_ = self.class_count_ / self.class_count_.sum()\n",
    "        return self\n",
    "    \n",
    "    def _joint_log_likelihood(self, X):\n",
    "        joint_log_likelihood = []\n",
    "        for i in range(np.size(self.classes_)):\n",
    "            jointi = np.log(self.class_prior_[i])\n",
    "            n_ij = - 0.5 * np.sum(np.log(2. * np.pi * self.sigma_[i, :]))\n",
    "            n_ij -= 0.5 * np.sum(((X - self.theta_[i, :]) ** 2) /\n",
    "                                 (self.sigma_[i, :]), 1)\n",
    "            joint_log_likelihood.append(jointi + n_ij)\n",
    "\n",
    "        joint_log_likelihood = np.array(joint_log_likelihood).T\n",
    "        return joint_log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9533333333333334"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "gnb = GaussianNB(priors=[.3, .2, .5])\n",
    "gnb.fit(iris.data, iris.target)\n",
    "y_pred = gnb.fit(iris.data, iris.target).predict(iris.data)\n",
    "#gnb.predict_proba(iris.data)\n",
    "(y_pred == iris.target).sum() / len(iris.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    " 类别不平衡问题\n",
    " \n",
    " 再缩放的思想虽简单,但实际操作却并不平凡,主要因为“训练集是真实样本总体的无偏采样”这个假设往往并不成立,也就是说,我们未必能有效地基于训练集观测几率来推断出真实几率.现有技术大体上有三类做法:第一类是直接对训练集里的反类样例进行“欠采样”(undersampling),即去除一些反例使得正、反例数目接近,然后再进行学习;第二类是对训练集里的正类样例进行“过采样”(oversampling)即增加一些正例使得正、反例数目接近,然后再进行学习;第三类则是直接基于原始训练集进行学习,但在用训练好的分类器进行预测时,将式(3.48)入到其决策过程中,称为“阈值移动”(threshold-moving)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDiscreteNB(BaseNB):\n",
    "    \"\"\"Abstract base class for naive Bayes on discrete/categorical data\n",
    "    Any estimator based on this class should provide:\n",
    "    __init__\n",
    "    _joint_log_likelihood(X) as per BaseNB\n",
    "    \"\"\"\n",
    "\n",
    "    def _update_class_log_prior(self, class_prior=None):\n",
    "        n_classes = len(self.classes_)\n",
    "        if class_prior is not None:\n",
    "            self.class_log_prior_ = np.log(class_prior)\n",
    "        elif self.fit_prior:\n",
    "            # empirical prior, with sample_weight taken into account\n",
    "            self.class_log_prior_ = (np.log(self.class_count_) -\n",
    "                                     np.log(self.class_count_.sum()))\n",
    "        else:\n",
    "            self.class_log_prior_ = np.full(n_classes, -np.log(n_classes))\n",
    "\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        \"\"\"Fit Naive Bayes classifier according to X, y\n",
    "        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
    "        y : array-like, shape = [n_samples]\n",
    "        sample_weight : array-like, shape = [n_samples], (default=None)\n",
    "            Weights applied to individual samples (1. for unweighted).\n",
    "        \"\"\"\n",
    "      \n",
    "        _, n_features = X.shape\n",
    "        labelbin = LabelBinarizer()\n",
    "        Y = labelbin.fit_transform(y)\n",
    "        self.classes_ = labelbin.classes_\n",
    "        if Y.shape[1] == 1:\n",
    "            Y = np.concatenate((1 - Y, Y), axis=1)\n",
    "\n",
    "        # LabelBinarizer().fit_transform() returns arrays with dtype=np.int64.\n",
    "        # We convert it to np.float64 to support sample_weight consistently;\n",
    "        # this means we also don't have to cast X to floating point\n",
    "        Y = Y.astype(np.float64)\n",
    "        if sample_weight is not None:\n",
    "            sample_weight = np.atleast_2d(sample_weight)\n",
    "            Y *= check_array(sample_weight).T\n",
    "\n",
    "        class_prior = self.class_prior\n",
    "\n",
    "        # Count raw events from data before updating the class log prior\n",
    "        # and feature log probas\n",
    "        n_effective_classes = Y.shape[1]\n",
    "        self.class_count_ = np.zeros(n_effective_classes, dtype=np.float64)\n",
    "        self.feature_count_ = np.zeros((n_effective_classes, n_features),\n",
    "                                       dtype=np.float64)\n",
    "        self._count(X, Y)\n",
    "        self._update_feature_log_prob(alpha)\n",
    "        self._update_class_log_prior(class_prior=class_prior)\n",
    "        return self\n",
    "\n",
    "    # XXX The following is a stopgap measure; we need to set the dimensions\n",
    "    # of class_log_prior_ and feature_log_prob_ correctly. for interpreting NB-model as a linear model.\n",
    "    def _get_coef(self):\n",
    "        return (self.feature_log_prob_[1:]\n",
    "                if len(self.classes_) == 2 else self.feature_log_prob_)\n",
    "\n",
    "    def _get_intercept(self):\n",
    "        return (self.class_log_prior_[1:]\n",
    "                if len(self.classes_) == 2 else self.class_log_prior_)\n",
    "\n",
    "    coef_ = property(_get_coef)\n",
    "    intercept_ = property(_get_intercept)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9533333333333334"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultinomialNB(BaseDiscreteNB):\n",
    "\n",
    "    def __init__(self, alpha=1.0, fit_prior=True, class_prior=None):\n",
    "        self.alpha = alpha\n",
    "        self.fit_prior = fit_prior\n",
    "        self.class_prior = class_prior\n",
    "\n",
    "    def _count(self, X, Y):\n",
    "        \"\"\"Count and smooth feature occurrences.\"\"\"\n",
    "        if np.any((X.data if issparse(X) else X) < 0):\n",
    "            raise ValueError(\"Input X must be non-negative\")\n",
    "        self.feature_count_ += safe_sparse_dot(Y.T, X)\n",
    "        self.class_count_ += Y.sum(axis=0)\n",
    "\n",
    "    def _update_feature_log_prob(self, alpha):\n",
    "        \"\"\"Apply smoothing to raw counts and recompute log probabilities\"\"\"\n",
    "        smoothed_fc = self.feature_count_ + alpha\n",
    "        smoothed_cc = smoothed_fc.sum(axis=1)\n",
    "\n",
    "        self.feature_log_prob_ = (np.log(smoothed_fc) -\n",
    "                                  np.log(smoothed_cc.reshape(-1, 1)))\n",
    "\n",
    "    def _joint_log_likelihood(self, X):\n",
    "        \"\"\"Calculate the posterior log probability of the samples X\"\"\"\n",
    "        return (np.dot(X, self.feature_log_prob_.T) +\n",
    "                self.class_log_prior_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env01(py36)",
   "language": "python",
   "name": "env01"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
